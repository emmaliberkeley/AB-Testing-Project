# Designing an Experiment
1. Key questions to ask: 
    1. What are the subjects? Units in the population that you are going to run tests on and comparing. 
    2. What is the population? Subjects in the US? Women?   
    3. What is the size of experiment?
    4. What is the duration of the experiment?
2. Unit of diversion: how we define an individual subject is in an experiment.  
    1. User ID: User login info, such as email address, user name. User could have multiple log in but user ID (login) is a good proxy for a user. All events corresponds to the same user ID are either in the control or exp group. For exmaple, the same user's activity on mobile app and web are all in the same group. Stable and consistent experience within the experiment. It is personally identifiable 
    2. Session/Event ID: Each session is assigned independently, allowing different experiences in separate sessions for the same user. Different from the User ID, Event ID will give users inconsistent experience and are not user visible. For exmaple, for ranked list, change the order of the list for difference event. Most users can't tell or won't notice. 
    3. Pageviews/Requests: Every page view or request is independently assigned, potentially varying experiences within the same session
    4. Cookie/Anonymous ID: Uses browser cookies for assigning users to variations, useful for non-logged-in users. Cookie ID changes as users switch devices or browsers. Users can choose to clear their cookies, so that the next time they visit the website they will assign a new cookie. Users can also change the preferences such that everytime you close your browser, it will be cleared automatically. Easier to clear cookie on a brower than an app
    5. Geography/Location: Diversion based on user's geographic location, ideal for region-specific features or content.
    6. Device ID: The device device id is typically something that's tied to a specific device, and it's unchangeable by the user. It's also considered identifiable because it's immutable. But it doesn't have the cross device or cross platform consistency that the user identifier might have.
    7. IP address: If the user changes location, then they often get a new IP address.![Alt text](<screenshots/Screenshot 2024-01-05 at 10.44.25 PM.png>)
        1. User ID: cannot assign user to a group based on their User ID before they sign in
        2. Cookie ID: they could clear their cookies at any point, hence the ? mark
        3. Event ID: on every single event, you'd re-decide whether that event was in the experiment group or the control group.
        4. Device ID: you don't typically have device id's for non-mobile devices, you wouldn't be able to run the experiment on events before the user switch to their mobile device
3. Consistency of Diversion (need consent when using User ID)
    1. IP based diversion may really be your only choice but may not get a clean comparison between your experiment and your control. For some internet service providers, all users connecting through modem dial-up might be assigned the same IP address. This aggregation can skew results. In an A/B test, if a large group of users from a single IP (representing many individual users) is put into either the control or experiment group, it creates an imbalance. This imbalance can lead to misleading conclusions about the test's impact.
    2. if you're testing a change that crosses the sign in, sign out border, then a user ID doesn't work as well. Use a cookie instead
    3. If the change is detectable and potentially have learn effect, usually use User ID or Cookie ID. If users can not notice the difference, use event ID. 
        1. Learn effect: Change of UI in the experiment group might struggle with finding certain functions or miss the familiarity of the old design. A short-term A/B test might conclude that the new UI is less effective, as it initially led to lower satisfaction. A longer-term test that accounts for the learning effect would reveal that user satisfaction improved over time as users adapted to the new UI.![Alt text](<screenshots/Screenshot 2024-01-06 at 4.18.04 PM.png>)Every time a user initiates a video, this action (event) is randomly assigned to either Group A or Group B. This assignment is independent for each video load event. A user could encounter both versions if they watch multiple videos. 
4. Unit of Analysis vs. Unit of Diversion
    1. Variability in A/B testing can be computed either empirically (based on observed data) or analytically (based on theoretical assumptions). The choice between these methods depends on the relationship between the unit of analysis and the unit of diversion.
    2. The unit of analysis is the denominator in the metric calculation, like page views in a click-through rate. When the unit of diversion (the entity at which participants are randomized) matches the unit of analysis (such as page views in event-based diversion), the analytically computed variability tends to be close to the empirically computed variability. However, when the unit of diversion is broader, like a cookie or user ID, the variability can be significantly higher, often by a factor of four, five, or more. This increase is due to the correlated nature of events within the same cookie or user ID, which violates the assumption of independence of each event used in analytical computations. In such cases, relying on empirically computed variability is recommended to accurately reflect the true variability in the data.
    3. If your unit of diversion is the user ID, multiple page views by the same user are more likely to exhibit similar behavior (either clicking or not clicking) compared to page views from different users. This behavioral similarity within the same user ID increases variability because it creates clusters of similar data points, which is not accounted for in the analytical model based on independent events.
    4. ![Alt text](<screenshots/Screenshot 2024-01-06 at 4.48.43 PM.png>)
        1. In the second case, the unit of analysis is cookie. However, each cookie can have multiple page views, which means that the same cookie can be in the both experiment and control group, or the unit of analysis is larger than the unit of diversion. When it's time to analyze the results, if you're looking at the user level (cookie), you might wrongly attribute changes in behavior to the treatment, when in fact, they are due to the user having experienced both versions. Usually we need the unit of diversion to be >= unit of analysis, meaning the same user ID(unit of diversion) can have multiple page views(unit of analysis). 
5. Inter- vs. Intra-User Experiments
    1. Inter-user vs intra-user experiments: The users only show up in one group or show up in both groups. Most AB testing is about inter group experiment. Inter-user experiments are subject to between-user variability. Intra-user experiments involve the same group of users experiencing all conditions or variants, one after the other
6. Target Population
    1. Run experient on affected traffic: Restrict to a certain geo, people who speak certain languages, age group, people using modern browser. When there are multiple experiments running at the same time, teams running different experiments will coordinate on which experiment should run in which location so that the result won't be influenced by other experiments. Also do not want the result to be diluted by global population. 
    2. Sometimes do not target population because it will influence 90% of the traffic or you don't know what population is going to be affected by your experiment. 
7. An example of experiment on New Zealand and other region, combined together as global data
    1. Since global population size is much larger than the New Zealand population, the SE_global is less than the SE_nz, which is usually be the case in practice. However, for the same amount of data points, the SE_global should be larger than the SE_nz since there are more variability in the global population and the New Zealand data is more uniform![Alt text](<screenshots/Screenshot 2024-01-06 at 9.54.24 PM.png>)![Alt text](<screenshots/Screenshot 2024-01-06 at 10.00.36 PM.png>)From the answer, adding the other population dilute the difference between the experiment and control. Thus, even if the SE_global is lower than SE_nz, the difference in NZ is significant while the difference in Global is not
8. Population vs. Cohort
    1. Cohorts are subsets of a population. They are selected based on specific criteria relevant to the study. For instance, a cohort might consist of people born in the same year or patients who have received a particular treatment. You want to use cohort when you want to see the users' historical change, such as learning effect, user retension, want to increase user activity. You are interested in how certain factors affect a group over time!
    2. ![Alt text](<screenshots/Screenshot 2024-01-06 at 10.19.01 PM.png>)The cohort selected is the users that started the lession after the experiment starts because the users who start the lession long time ago might have finished the lession already. The experiment and control should be all selected from the cohort. 
9. Sizing
    1. If you want to see the effect of improving latency on user experience with User ID as unit of diversion, you will need to collect a large amount of data before you know if the effort worth it. Thus you can narrow down the scope of your experiment by only looking at the users that are very active in the past two months with slow internet to get a sense whether the experiment is making sense
    2. Example: https://video.udacity-data.com/topher/2016/December/5845e980_empirical-sizing/empirical-sizing.r ![Alt text](<screenshots/Screenshot 2024-01-06 at 10.42.21 PM.png>)![Alt text](<screenshots/Screenshot 2024-01-06 at 10.42.42 PM.png>)
        1. the main take away from standard error is proportional to 1/sqrt(N) where N is the sample size. The rate at which the standard error decreases is not linear but follows a square root function. Increasing the sample size has a more significant impact when the sample size is small. However, once you have a sufficiently large sample, increasing it further only yields marginal decreases in standard error. A smaller standard error means that the sample mean is a more accurate estimate of the population mean. This increases the statistical confidence in the results, making it easier to detect true effects or differences in data.
    3. ![Alt text](<screenshots/Screenshot 2024-01-07 at 11.16.33 AM.png>)
    ![Alt text](<screenshots/Screenshot 2024-01-07 at 11.39.44 AM.png>)
        1. Lower variability (SD) needs lower sample size: With lower variability, it's easier to detect a difference between groups. Smaller differences become statistically significant because the data points are more tightly clustered, making any deviation more noticeable. High variability requires a larger sample to distinguish the effect from random noise. Conversely, low variability means the true effect can be detected with fewer data points.
        2. Non-English traffic are not effected, including them will dilute the effect, which will increase the number of page views needed. The effect on English speaking users are more pronounced than other groups, making it easier to detect the effect and require less sample size. Since you are looking at only a subset of your users, you might want a bigger change before it matters to the business (higher practical significance boundry), the smallest effect size that is considered meaningful or important for practical purposes. This means you would be looking for larger differences or changes to ensure that the findings are not only statistically significant but also practically relevant and not due to random variation. Or it's likely that the variability will go down and you can take advantage of that, meaning without decreasing the sample size, you can detect smaller changes. Since the practical significance boundry can more in either direction, you size can also move in either direction. But it is more likely that size goes down and practical significance boundtry goes up
        3. The unit of diversion and unit of analysis is the same, so sample size might decrease but not going to help that much
10. Duration vs. Exposure (traffic proportion)
    1. Translating Ideal Size to Practical Decisions: After deciding on the ideal size of an experiment, practical considerations include the experiment's duration, timing, and the fraction of traffic to include.
    2. Determining Experiment Duration: The duration depends on the daily visitor count (cookies) and the percentage of traffic allocated to the experiment and control.
        1. Example: If you need 1 million cookies in total and receive 100,000 daily, running 50% of traffic through each group (experiment and control) means a duration of 10 days. Reducing to 25% each for the experiment and control doubles the duration to 20 days
    3. Timing of the Experiment: Consideration of external factors like holidays and back-to-school seasons that might influence user behavior and skew results.
    4. Traffic Allocation Decisions: Allocating only a portion of traffic to the experiment instead of 100% can be due to safety concerns (testing a new UI feature’s functionality and user reception), or to avoid premature publicity if the feature might not be permanent
    5.  Accounting for Variability: Running multiple experiments simultaneously at smaller traffic percentages ensures comparability, as each test is equally affected by external variables like holidays or traffic shifts.
    6. Example![Alt text](<screenshots/Screenshot 2024-01-07 at 11.55.12 AM.png>)
        1. Each day does not have the same traffic, weekend lower than 500,000 page views. You need at least 3 days a mix of week day and weekend
        2. Run longer with lower exposure (traffic proportion)
        ![Alt text](<screenshots/Screenshot 2024-01-07 at 11.59.04 AM.png>)
11. Learning effects
    1. Learning effects occur when users adapt to changes over time, showing either change aversion (initial resistance) or novelty effects (initial excitement). Impact Over Time: Initially, users may react strongly (either positively or negatively), but their behavior often plateaus to a different, more stable pattern over time.
    2. Measuring Learning Effect
        1. Time Factor: Measuring learning effects requires time, as users need a duration to adapt to changes.
        2. Unit of Diversion: A stateful unit of diversion, like a cookie or user ID, is essential for measuring user learning, ensuring consistent exposure to the change.
        3. Risk and Duration: High-risk changes or those with uncertain effects should be tested on a small portion of users over a longer period.
    3. Methodological Approaches
        1. Pre-Periods and Post-Periods: Google uses methodologies involving pre-periods and post-periods for their experiments.Pre-Periods: Before the A/B test, an A vs. A test is run on the same populations to ensure no inherent differences affect the outcome.
        2. Post-Periods: After the A/B test, another A vs. A test checks for any differences attributable to user learning during the experiment. Attributing Differences to Learning: Post-periods allow for attributing any observed differences post-experiment to the learning that occurred during the test period.
    4. Advanced Techniques: These methods are advanced and most suitable for organizations with extensive experience in running experiments. For those new to experimentation, simpler techniques are recommended.






    